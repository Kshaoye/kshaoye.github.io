import{_ as r,c as t,o,ag as a}from"./chunks/framework.z0p7P-Mv.js";const l="/img/k8s/kube-scheduler-procedure.jpg",h=JSON.parse('{"title":"Kubernetes核心组件","description":"","frontmatter":{},"headers":[],"relativePath":"zh/k8s/KubernetesCoreComponents.md","filePath":"zh/k8s/KubernetesCoreComponents.md"}'),i={name:"zh/k8s/KubernetesCoreComponents.md"};function s(u,e,n,c,p,d){return o(),t("div",null,[...e[0]||(e[0]=[a('<h1 id="kubernetes核心组件" tabindex="-1">Kubernetes核心组件 <a class="header-anchor" href="#kubernetes核心组件" aria-label="Permalink to &quot;Kubernetes核心组件&quot;">​</a></h1><h2 id="kube-apiserver" tabindex="-1">kube-apiserver <a class="header-anchor" href="#kube-apiserver" aria-label="Permalink to &quot;kube-apiserver&quot;">​</a></h2><ul><li>整个k8s集群的<strong>访问入口</strong></li><li>提供k8s各类资源对象的增删改查及watch等HTTPS Rest接口</li><li>相关资源对象包括pods、services、replicationcontrollers等</li><li>API Server为REST操作提供服务，并为集群的共享状态提供前端</li><li>所有其他组件都通过该前端进行交互</li></ul><h3 id="kubernetes-api-server简介" tabindex="-1">kubernetes API Server简介 <a class="header-anchor" href="#kubernetes-api-server简介" aria-label="Permalink to &quot;kubernetes API Server简介&quot;">​</a></h3><ul><li><p>默认端口：6443（可以通过启动参数“--secure-port&quot;修改）</p></li><li><p>默认监听IP为0.0.0.0及本机所有IP（可以通过启动参数“--bind-address&quot;修改）</p></li><li><p>接收来自客户端、dashboard等外部Https请求</p></li><li><p>实现基于Tocken文件或客户端证书及HTTP Base的认证</p></li><li><p>实现基于策略的账户鉴权及准入</p></li><li><p>客户端通过API Server远程调用kubernetes的API远程，实现对kubernetes内部资源的增删改查等管理任务的分发。</p></li></ul><h3 id="api的版本" tabindex="-1">API的版本 <a class="header-anchor" href="#api的版本" aria-label="Permalink to &quot;API的版本&quot;">​</a></h3><ul><li>Alpha：预览版，可能包含bug或错误，不建议使用。</li><li>Beta：公测版，可能存在不稳定或潜在bug，不建议生产环境使用</li><li>v1/v2/vX：稳定版，适合生产环境使用</li></ul><p>可以通过curl测试API，例如：</p><p><code>#curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/ #返回所有的API列表</code><code># curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/apis #分组API</code><code># curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/api/v1 #带具体版本号的API</code><code># curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/version #API版本信息</code><code># curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/healthz/etcd #与etcd的心跳监测</code><code># curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/apis/autoscaling/v1 #指定API的详细信息</code><code># curl --cacert /etc/kubernetes/ssl/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.31.7.101:6443/metrics #指标数据</code></p><h2 id="etcd" tabindex="-1">etcd <a class="header-anchor" href="#etcd" aria-label="Permalink to &quot;etcd&quot;">​</a></h2><p>目前是Kubernetes默认使用的key-value数据存储系统，用于<strong>保存kubernetes的所有集群数据</strong>，etcd支持分布式集群功能，生产环境使用时<strong>需要为etcd数据提供定期备份机制</strong>。</p><h2 id="kube-scheduler" tabindex="-1">kube-scheduler <a class="header-anchor" href="#kube-scheduler" aria-label="Permalink to &quot;kube-scheduler&quot;">​</a></h2><p>负责将 Pods 按照一定的<strong>调度</strong>策略指派到目的节点上</p><ul><li><p>通过调度算法为待调度Pod列表的每个Pod从可用Node列表中选择一个最适合的Node，并将信息写入etcd中。</p></li><li><p>node节点上的kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载Image并启动容器。</p><p><img src="'+l+'" alt="kube-scheduler调度过程"></p></li></ul><h2 id="kube-controller-manager" tabindex="-1">kube-controller-manager <a class="header-anchor" href="#kube-controller-manager" aria-label="Permalink to &quot;kube-controller-manager&quot;">​</a></h2><ul><li><strong>控制器管理器</strong>，包括一些子控制器(副本控制器、节点控制器、命名空间控制器和服务账号控制器等)</li><li>控制器作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理</li><li>当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群中的pod副本始终处于预期的工作状态。 基于controller-manager实现的pod高可用机制：</li><li>controller-manager控制器每间隔5秒检查一次节点的状态。 （node monitor period：节点监视周期，5s）</li><li>如果controller-manager控制器没有收到自节点的心跳，则将该node节点被标记为不可达。</li><li>controller-manager将在标记为无法访问之前等待40秒。 (node monitor grace period：节点监视宽限期，40s)</li><li>如果该node节点被标记为无法访问后5分钟还没有恢复，controller-manager会删除当前node节点的所有pod并在其它可用节点重建这些pod。 （pod eviction timeout：pod超时驱逐时间,5m）</li><li>Controller：负责把用户通过API Server中遵循 某个特定API实例化的对象，生产出来；</li><li>智能程序：专用于特定领域</li></ul><h2 id="kube-proxy" tabindex="-1">kube-proxy <a class="header-anchor" href="#kube-proxy" aria-label="Permalink to &quot;kube-proxy&quot;">​</a></h2><p>​ Kubernetes网络代理，运行在 node 上，反映了 node上 Kubernetes API中定义的服务，并可以通过一组后端进行简单的 TCP、UDP和 SCTP 流转发或者在一组后端进行循环 TCP、UDP 和 SCTP 转发。 ​ 用户必须使用 apiserver API 创建一个服务来配置代理（其实就是kube-proxy通过在主机上维护网络规则并执行连接转发来实现Kubernetes服务访问）。 kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过管理 IPtables或者IPVS规则 来实现网络的转发。</p><h3 id="kube-proxy的三种工作模式" tabindex="-1">Kube-Proxy的三种工作模式 <a class="header-anchor" href="#kube-proxy的三种工作模式" aria-label="Permalink to &quot;Kube-Proxy的三种工作模式&quot;">​</a></h3><ul><li><p>UserSpace：从k8s 1.2开始，已淘汰。</p></li><li><p>IPtables：早期使用，节点数量时多会遇到瓶颈</p></li><li><p>IPVS：从k8s 1.9开始引入，1.11之后默认使用IPVS，默认使用轮询调度</p><p>IPVS 相对 IPtables 效率会更高一些，使用 IPVS 模式需要在运行 Kube-Proxy 的节点上安装 ipvsadm、ipset 工具包和加载 ip_vs内核模块，当 Kube-Proxy 以 IPVS 代理模式启动时，Kube-Proxy 将验证节点上是否安装了 IPVS 模块，如果未安装，则 Kube-Proxy将回退到 IPtables 代理模式。</p></li></ul><h2 id="kublet" tabindex="-1">kublet <a class="header-anchor" href="#kublet" aria-label="Permalink to &quot;kublet&quot;">​</a></h2><p>kubelet是运行在每个worker节点的代理组件，它会监视已分配给节点的pod。</p><p>具体功能如下：</p><ul><li><p>向master汇报node节点的状态信息</p></li><li><p>接受指令并在Pod中创建 docker容器</p></li><li><p>准备Pod所需的数据卷</p></li><li><p>返回pod的运行状态</p></li><li><p>在node节点执行容器健康检查</p></li></ul><h2 id="kubectl" tabindex="-1">kubectl <a class="header-anchor" href="#kubectl" aria-label="Permalink to &quot;kubectl&quot;">​</a></h2><p>通过命令行对kubernetes集群进行管理的客户端工具。</p>',26)])])}const k=r(i,[["render",s]]);export{h as __pageData,k as default};
